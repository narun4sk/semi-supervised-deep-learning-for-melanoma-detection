{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there are no import errors - the environment was set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will download (if needed) and set up MNIST datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training* dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./\n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = datasets.MNIST(\"./\", train=True, download=True)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then *Validation* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./\n",
       "    Split: Test"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = datasets.MNIST(\"./\", train=False, download=True)\n",
    "valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the elements in `torchvision` datasets are tuples, where the first item is `PIL.Image.Image`, the second - a numeric value which is presented in the image. Although datasets do have `data` and `targets` attributes, which are `tensors` by themselves, `DataLoader` takes as an input dataset, where the first item in each element must be tensor. Therefore it's best to convert them to tensors during the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F69ED233E50>, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set[3600][0], cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, tensor(8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.data), type(train_set.data[3600]), train_set.targets[3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set.data[3600], cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_set = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=196, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=196, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_set[3600]\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _deep-learning_ model is a directed, acyclic graph of layers. A first layer will learn small local patterns such as edges, a second layer will learn larger patterns made of the features of the first layer, and so on.\n",
    "\n",
    "The most common instance is a linear stack of layers, mapping a single input to a single output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=196, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=196, out_features=98, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=98, out_features=10, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 196), # input size = 28x28 = 784\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(196, 98),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(98, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here _ReLU is **activation function**_ (also called a non-linearity). Without it the Dense layer\n",
    "would consist of two linear operations - a dot product and an addition (output = dot(W, input) + b).\n",
    "So the layer could only learn linear transformations of the input data: the hypothesis space (a space of possibilities) of the layer would be the set of all possible linear transformations of the input data. Such a hypothesis space is too restricted and wouldn't benefit from multiple layers of representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Loss function** (objective function)_ - the quantity that will be minimized during training. Represents a measure of success for the task at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Optimizer*** - determines how the network will be updated based on the _loss function_. It implements a specific variant of stochastic gradient descent (SGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, train_loss: 0.01546, valid_loss: 0.01929, train_acc: 0.89628, valid_acc: 0.87140\n",
      "Epoch: 02, train_loss: 0.00719, valid_loss: 0.02557, train_acc: 0.95453, valid_acc: 0.84720\n",
      "Epoch: 03, train_loss: 0.00528, valid_loss: 0.00475, train_acc: 0.96612, valid_acc: 0.96800\n",
      "Epoch: 04, train_loss: 0.00411, valid_loss: 0.00698, train_acc: 0.97427, valid_acc: 0.95410\n",
      "Epoch: 05, train_loss: 0.00332, valid_loss: 0.00852, train_acc: 0.97915, valid_acc: 0.94330\n",
      "Epoch: 06, train_loss: 0.00276, valid_loss: 0.01615, train_acc: 0.98318, valid_acc: 0.89080\n",
      "Epoch: 07, train_loss: 0.00235, valid_loss: 0.00672, train_acc: 0.98582, valid_acc: 0.95670\n",
      "Epoch: 08, train_loss: 0.00205, valid_loss: 0.00314, train_acc: 0.98767, valid_acc: 0.97990\n",
      "Epoch: 09, train_loss: 0.00171, valid_loss: 0.00761, train_acc: 0.99028, valid_acc: 0.94780\n",
      "Epoch: 10, train_loss: 0.00155, valid_loss: 0.00335, train_acc: 0.99065, valid_acc: 0.97750\n",
      "Epoch: 11, train_loss: 0.00133, valid_loss: 0.00613, train_acc: 0.99218, valid_acc: 0.96140\n",
      "Epoch: 12, train_loss: 0.00114, valid_loss: 0.00308, train_acc: 0.99370, valid_acc: 0.97960\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(12):\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    train_acc = 0.\n",
    "    valid_acc = 0.\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        # Convert target to one-hot encoding (because of using the MSELoss)\n",
    "        target = torch.zeros(data.size(0), 10).scatter_(1, target[:, None], 1.)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.view(data.size(0), -1))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (torch.argmax(output, 1) == torch.argmax(target, 1)).float().sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            target = torch.zeros(data.size(0), 10).scatter_(1, target[:, None], 1.)\n",
    "        \n",
    "            output = model(data.view(data.size(0), -1))\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += (torch.argmax(output, 1) == torch.argmax(target, 1)).float().sum()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "    train_acc /= len(train_set)\n",
    "    valid_acc /= len(valid_set)\n",
    "   \n",
    "    print('Epoch: {:0>2}, train_loss: {:.5f}, valid_loss: {:.5f}, train_acc: {:.5f}, valid_acc: {:.5f}'.format(\n",
    "        epoch + 1, train_loss, valid_loss, train_acc, valid_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
